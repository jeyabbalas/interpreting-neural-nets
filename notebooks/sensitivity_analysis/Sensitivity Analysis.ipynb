{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Variable Importance using Sensitivity Analysis\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "1. The Jacobian ($\\pmb{J}$) is the partial derivative of the of the outcome variable ($y$) with respect to each input variable ($x_i; \\text{ where } i=1,\\cdots, n$). It is made adimensional by multiplying with the predictor value by the output value.\n",
    "\n",
    "$$\n",
    "\\pmb{J}_{\\vec{x}} = \\left[\\frac{\\partial f(\\vec{x})}{\\partial x_{1}}\\cdot\\frac{x_{1}}{y}\\;\\;\\;\\;\\;\\; \\frac{\\partial f(\\vec{x})}{\\partial x_{2}}\\cdot\\frac{x_{2}}{y}\\;\\; \\cdots\\;\\; \\frac{\\partial f(\\vec{x})}{\\partial x_{n}}\\cdot\\frac{x_{n}}{y}\\right]\n",
    "$$\n",
    "\n",
    "2. Compute the absolute value of the Jacobian for all the instances in the training dataset ($D^{train}$). This corresponds to the variable importance of each predictor variable according to the model induced from the training dataset.\n",
    "\n",
    "$$\n",
    "VarImp = \\frac{1}{|D^{train}|}\\sum_{\\vec{x} \\in D^{train}} |\\pmb{J}_{\\vec{x}}|\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./data/iris_train.csv\")\n",
    "train_data_np = train_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the ANN model\n",
    "model = model_from_json(open('./model/model_architecture.json').read())\n",
    "model.load_weights('./model/model_weights.h5')\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_adimensional_jacobian_1output(x, model):\n",
    "    \"\"\"\n",
    "    Computes the adimensional Jacobian of an input vector and \n",
    "    a Keras model with a single output node.\n",
    "    \n",
    "    Input—\n",
    "    x: Floating numpy vector of inputs.\n",
    "    model: Keras model.\n",
    "    \n",
    "    Output—\n",
    "    jacobian: Absolute adimensional numpy Jacobian vector for \n",
    "        scalar output wrt each input.\n",
    "    \"\"\"\n",
    "    x_tensor = tf.convert_to_tensor(x.reshape(1,-1), dtype=tf.float32)\n",
    "    with tf.GradientTape() as g:\n",
    "        g.watch(x_tensor)\n",
    "        y_tensor = model(x_tensor)\n",
    "    jacobian = g.jacobian(y_tensor, x_tensor)\n",
    "    jacobian = jacobian.numpy()[0][0][0]\n",
    "    \n",
    "    # Hadamard product between input-variable-gradient and \n",
    "    # variable-value/loss to remove the dimensions\n",
    "    input_by_loss = x/y_tensor.numpy()[0][0]\n",
    "    adim_jacobian = np.multiply(jacobian, input_by_loss)\n",
    "    \n",
    "    return np.absolute(adim_jacobian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0222397 , 0.06126562, 0.03610831, 0.00580287])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs_adimensional_jacobian_1output(np.array([5,2,1,0.2]), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_importance_sensitivity_analysis(data, model):\n",
    "    \"\"\"\n",
    "    Computes variable importance of each input variable using sensitivity analysis.\n",
    "    \n",
    "    Input—\n",
    "    data: Data as a numpy matrix.\n",
    "    model: Keras model.\n",
    "    \n",
    "    Output—\n",
    "    variable_importance: average absolute gradient wrt each input over all instances\n",
    "    of 'data'.\n",
    "    \"\"\"\n",
    "    abs_jacobian = np.apply_along_axis(abs_adimensional_jacobian_1output,\n",
    "                       1,\n",
    "                       data, model=model)\n",
    "    sum_jacobian = np.sum(abs_jacobian, axis=0)\n",
    "    \n",
    "    return sum_jacobian/data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.12177903, 1.97653917, 6.43293472, 1.40579731])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variable_importance_sensitivity_analysis(train_data_np, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Novelty Index using Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Novelty Index Algorithm\n",
    "\n",
    "1. Get weights from **sensitivity analysis**. The relative importance of each input variable $x_{i}$ is proportional to its non-dimensional gradient in the neural network. The gradient is the partial derivative of the network output ($y$) with respect to the input variable $x_{i}$.\n",
    "\n",
    "$$\n",
    "\\vec{S}_{\\vec{x}} = \\frac{\\partial f(\\vec{x})}{\\partial x_{i}}\\cdot \\frac{x_{i}}{y};\\;\\;\\;\\;\\;\\; \\forall i \\in \\vec{x}\n",
    "$$\n",
    "\n",
    "2. Weigh instances by their sensitivity values. Project all the training instances ($x^{j}; j=1,\\cdots m$) and the test instance ($x^{t}$), weighed by their *absolute* sensitivity weights ($|S_{x_{i}}|; i=1\\cdots n$).\n",
    "\n",
    "$$\n",
    "\\vec{x}^{\\,j} := |\\vec{S}_{\\vec{x}^{\\,j}}| \\circ \\vec{x}^{\\,j};\\;\\;\\;\\;\\;\\; \\text{ where }j=1,\\cdots,m,t.\n",
    "$$\n",
    "\n",
    "3. Compute the smallest Euclidean distance between the weighted test instance and any weighted train instance.\n",
    "\n",
    "$$\n",
    "d_{min}(\\vec{x}^{\\,t},D^{train}) = \\min_{j} \\sqrt{\\sum_{i=1}^{n} (\\vec{x}^{\\,j} - \\vec{x}^{\\,t})^2};\\;\\;\\;\\;\\;\\; \\forall \\vec{x}^{\\,j} \\in D^{train}\n",
    "$$\n",
    "\n",
    "4. Compute the median Euclidean distance between each training instance with every other training instance.\n",
    "\n",
    "$$\n",
    "d_{median}(D^{train}) = median \\left( d_{min}\\left(\\vec{x}^{\\,j}, D^{train}\\setminus\\vec{x}^{\\,j}\\right) \\right);\\;\\;\\;\\;\\;\\; \\forall \\vec{x}^{\\,j} \\in D^{train}\n",
    "$$\n",
    "\n",
    "5. Calculate Novelty Index ($\\eta$), which is the ratio between the smallest Euclidean distance between the test instance and all train instances ($d_{min}$), and the median Euclidean distance between each training instance with every other training instance ($d_{median}$).\n",
    "\n",
    "$$\n",
    "\\eta(\\vec{x}^{\\,t}, D^{train}) = \\frac{d_{min}(\\vec{x}^{\\,t},D^{train})}{d_{median}(D^{train})}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weigh_instance_by_sensitivity(x, model):\n",
    "    \"\"\"\n",
    "    Weighs an input numpy vector for the Keras model by its sensitivity.\n",
    "    \n",
    "    Input—\n",
    "    x: Floating numpy vector of inputs.\n",
    "    model: Keras model.\n",
    "    \n",
    "    Output—\n",
    "    weighted_x: Input x weighted by its sensitivity weight.\n",
    "    \"\"\"\n",
    "    sens_wts = abs_adimensional_jacobian_1output(x, model)\n",
    "    # Hadamard product between absolute sensitivity-weights and query\n",
    "    weighted_x = np.multiply(sens_wts, x)\n",
    "    \n",
    "    return weighted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.39747321,  2.36322359, 15.75715986,  3.9102108 ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weigh_instance_by_sensitivity(np.array([1,2,3,4]), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weigh_matrix_by_sensitivity(reference, model):\n",
    "    \"\"\"\n",
    "    Weighs all rows of the reference data matrix by its sensitivity in the \n",
    "    input Keras model.\n",
    "    \n",
    "    Input—\n",
    "    reference: Input data matrix to be weighted by sensitivity.\n",
    "    \n",
    "    Output— \n",
    "    Input data matrix weighed by sensitivity.\n",
    "    \"\"\"\n",
    "    return np.apply_along_axis(weigh_instance_by_sensitivity, \n",
    "                               1, \n",
    "                               reference, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_weighted_euclidean_dist(query, reference, model):\n",
    "    \"\"\"\n",
    "    Compute the euclidean distance between the query (weighted by the sensitivity)\n",
    "    and each data point in reference (weighed by their sensitivities), then return\n",
    "    the minimum of the distances.\n",
    "    \n",
    "    Input—\n",
    "    query: Data point queried to the Keras model.\n",
    "    reference: Training data or any representation of training data\n",
    "        used to build the input model.\n",
    "    model: Keras model.\n",
    "    \n",
    "    Output—\n",
    "    min_dist: minimum Euclidean distance between weighted query and weighted\n",
    "        reference data points.\n",
    "    \"\"\"\n",
    "    weighted_query = weigh_instance_by_sensitivity(query, model)\n",
    "    weighted_ref = weigh_matrix_by_sensitivity(reference, model)\n",
    "    dists = np.apply_along_axis(np.linalg.norm, \n",
    "                                1, \n",
    "                                (weighted_ref-weighted_query))\n",
    "    min_dist = min(dists)\n",
    "    return min_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_min_weighted_euclidean_dist(query, reference, model):\n",
    "    \"\"\"\n",
    "    Compute the minimum Euclidean distance between weighted query and weighted\n",
    "    reference datapoints excluding the distance comparing query to itself.\n",
    "    \n",
    "    Input—\n",
    "    query: Data point from within the reference set.\n",
    "    reference: Training data or any representation of training data\n",
    "        used to build the input model.\n",
    "    model: Keras model.\n",
    "    \n",
    "    Output—\n",
    "    second_min_dist: Min euclidean distance between query and all reference\n",
    "        data points excluding the query.\n",
    "    \"\"\"\n",
    "    weighted_query = weigh_instance_by_sensitivity(query, model)\n",
    "    weighted_ref = weigh_matrix_by_sensitivity(reference, model)\n",
    "    dists = np.apply_along_axis(np.linalg.norm, \n",
    "                                1, \n",
    "                                (weighted_ref-weighted_query))\n",
    "    \n",
    "    # because the min would be the instance compared to itself\n",
    "    second_min_dist = np.sort(dists, \n",
    "                              kind=\"mergesort\")[1]\n",
    "    return second_min_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_ref_wt_euclidean_dist(reference, model):\n",
    "    \"\"\"\n",
    "    Median euclidean distance between every data point in the reference\n",
    "    compared to every other data point in the reference, each weighted\n",
    "    by their sensitivities.\n",
    "    \n",
    "    Input—\n",
    "    reference: data representing the training data that model was trained on.\n",
    "    model: Keras model.\n",
    "    \n",
    "    Output—\n",
    "    Median euclidean distance between each data point in reference compared \n",
    "        to every other data point.\n",
    "    \"\"\"\n",
    "    min_dists = np.apply_along_axis(second_min_weighted_euclidean_dist, \n",
    "                        1, reference, \n",
    "                        reference=reference,\n",
    "                        model = model)\n",
    "    return np.median(min_dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_forbidden_zone(query):\n",
    "    \"\"\"\n",
    "    Tells if the query is unintelligible for the Keras model.\n",
    "    \n",
    "    Input—\n",
    "    query: Query to be used for the Keras model\n",
    "    \n",
    "    Output—\n",
    "    Boolean True if the query is legal for the Keras model,\n",
    "        False otherwise.\n",
    "    \n",
    "    \"\"\"\n",
    "    return np.any(query<0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_novelty_index(query, nov_deno, reference, model):\n",
    "    \"\"\"\n",
    "    Computes novelty index.\n",
    "    \n",
    "    Input—\n",
    "    query: Query to be used for the Keras model as a numpy row vector.\n",
    "    nov_deno: Median euclidean distance between each data point in reference compared \n",
    "        to every other data point in the reference.\n",
    "    reference: Representation of training data.\n",
    "    model: Keras model.\n",
    "    \n",
    "    Output—\n",
    "    Novelty index\n",
    "    \n",
    "    \"\"\"\n",
    "    if(in_forbidden_zone(query)):\n",
    "        return np.inf\n",
    "    \n",
    "    nov_nume = min_weighted_euclidean_dist(query, reference, model)\n",
    "    \n",
    "    return nov_nume/nov_deno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "deno = median_ref_wt_euclidean_dist(train_data_np, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7835123384693827"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.412492114194928"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = np.array([5, 2, 10.2, 0.2]).reshape(1,-1)\n",
    "compute_novelty_index(query, deno, train_data_np, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of Sensitivity Analysis\n",
    "1. The model $f(x)$ is typically non-linear. So, sensitivity depends upon the input. In some regions the sensitivity for the same variable-value may be large, and small in others.\n",
    "    1. Averaging the sensitivity provides a possible approximation of its variable importance.\n",
    "2. Input variables have different scales.\n",
    "    1. Standardize them or make them adimensional.\n",
    "3. Sensitivity tells us about the model, not the underlying data. If two input variables are highly correlated, the model may prefer one over the other.\n",
    "    1. One possible solution is to use Dropouts while training model, so the model generalizes better.\n",
    "4. Partial derivatives do not have any meaning for categorical variables. Only applies to continuous variables, which is a theoretical concept. In practice, all variable measurements are discrete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
